---
layout:     post                    # 使用的布局（不需要改）
title:     bolt源码梳理(上) --核心结构分析               # 标题 
#subtitle:    #副标题
date:       2018-08-24              # 时间
author:     BY  Seaside                    # 作者
#header-img: img/blog/17.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - boltdb
    - database
---

# 1. 简介

`boltdb`是一款golang实现的嵌入式K-V存储引擎。在boltdb的源码中的[doc.go](https://github.com/boltdb/bolt/blob/master/doc.go) 对其有一个简要的说明。 其主要设计源于Howard Chu的LMDB。

主要特性：

- 事务，允许多个读事务, 但是只允许一个写事务
- ACID语言
- 无锁MVCC支持
- 数据提供零拷贝
- B-TREE索引。

> Bolt is a single-level, zero-copy, B+tree data store. This means that Bolt is
> optimized for fast read access and does not require recovery in the event of a
> system crash. Transactions which have not finished committing will simply be
> rolled back in the event of a crash.

实际场景中可以使用boltdb在本地存储一些用户数据。在boltdb中，所有的数据都存储在bucket中，这个bucket就相当于是一个空间，分割了不同的数据。为了增加层次性，在一个bucket中甚至可以存储另一个bucket，所以站在boltdb的角度来看，bucket中只能存储两种类型的数据，一种是`[]byte`，另一种是`bucket`(其实在最底层，存储的都是[]byte)。

## 1.1 项目结构

从gitbhub上clone完[BoltDB](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2Fboltdb)的代码后，我们可以发现，它主要包含下列文件:

- bucket.go, cursor.go, db.go, freelist.go, node.go, page.go, tx.go --- 这些文件是BoltDB实现的核心，分别定义和实现了Transaction、Bucket及B+ Tree等机制，也是我们重点阅读与分析的代码。
- bucket_test.go, cursor_test.go,db_test.go, freelist_test.go, node_test.go, page_test.go, quick_test.go, simulation_test.go, tx_test.go --- 对应的测试文件。
- bolt_linux.go, bolt_openbsd.go, bolt_ppc.go, bolt_unix.go, bolt_unix_solaris.go, bolt_windows.go, boltsync_unix.go --- 这些文件封装了对应平台下的mmap、fdatasync、flock相关的系统调用。BoltDB只生成一个数据库文件，并通过mmap对文件进行只读映射，写时通过write和fdatasync系统调用修改文件。Go以文件名加"_"，再加"GOOS"或者“GOARCH”的形式或者源文件起始位置添加“// +build”的形式实现不同平台的条件编译。
- bolt_386.go, bolt_amd64.go, bolt_arm.go, bolt_arm64.go, bolt_s390x.go,  bolt_ppc.go, bolt_ppc64.go, bolt_ppc64le.go --- 这些文件定义了对应ABI平台下mmap映射文件大小的上限maxMapSize、分配数组的最大长度maxAllocSize以及是否支持非对齐内存访问([unaligned memory access](https://link.jianshu.com?t=https%3A%2F%2Fwww.kernel.org%2Fdoc%2FDocumentation%2Funaligned-memory-access.txt))。
- errors.go --- 定义了BoltDB的错误类型及对应的提示字符。
- doc.go - 包bolt的说明文档。

# 2. 使用入门

打开db文件

```go
//打开或创建一个文件，权限为600
db, err := bolt.Open("tmp.db", 0600,nil)
```

在db下创建一个bucket

```go
db.Update(func(tx *bolt.Tx) error {
    _, err := tx.CreateBucket([]byte("test_bucket"))
    return err
})
```

打开bucket并写入值

```go
db.Update(func(tx *bolt.Tx) error {
    b := tx.Bucket([]byte("test_bucket"))
    b.Put([]byte("key"), []byte("value"))
    v := b.Get([]byte("key"))
    fmt.Println(string(v))
    return  nil
})
```

只读模式打开并读取值

```go
db.View(func(tx *bolt.Tx) error {
    b := tx.Bucket([]byte("test_bucket"))
    v := b.Get([]byte("key"))
    fmt.Println(string(v))
    return  nil
})
```

从这几个简单的小栗子可以看到，Update和View是数据库的两个主要方法，但Update所做的改变是会写入到文件的（只要Update内部的匿名函数返回nil），而在View的内部所做的操作并不会被写入文件，所以View的内部也可以使用Put方法，但不会生效。其实看到这里这个boltdb的一般使用的方法已经可以完全掌握了，无非就是Put/Get和创建Bucket读取Bucket。

# 3. boltdb结构

## 3.1 文件结构

从上面的示例代码中读者应该可以看出来我们使用了Tx，而Tx代表了事务，其特点是要么做完，要么不做（而View方法的特点是要么不做，要么还是不做）。所以可以推断出，程序所做的操作其实是马上写入到了文件进行了持久化的。大家都知道IO操作是一个很耗时间的操作，而且对于机械硬盘而言随机访问的耗时是非常巨大的。所以boltdb采用了分页(page)式访问，默认情况下一个页的大小为4k，每次当从文件中读取和写入都是以页作为最小的基本单位。除此之外boltdb也需要将相关性比较强的数据（同一个bucket中的数据）尽量写在同一个page中以避免磁盘随机访问带来的性能损耗。为了加快读取速度，boltdb也需要在内存中对磁盘上的数据进行缓存。

总结一下，boltdb的数据结构需要满足以下特点：

- 分页访问，整读整存
- 文件存储区域尽量集中，减少随机读取
- 内存缓存文件中的内容

默认情况下，当创建一个db文件的时候默认会写入4个page，前两个是meta page，第三个是freelist page，最后一个是leaf page，如下图所示

![](<https://ryanyux.github.io/post/go-blotdb-source/db.jpg>)



## 3.2 数据结构

### 3.2.1 核心概念

- [`Options`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/db.go#L897-L922) 初始化`boltdb`时的相关配置选择；
- [`DB`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/db.go#L45-L130) 整个`boltdb`的持有者，跟`boltdb`相关操作都要通过调用其方法发起，是`boltdb`的一个抽象；
- [`Stats`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/db.go#L932-L944) 调用`DB.Stats()`方法返回的数据结构，内包含一些`boltdb`内部的计数信息，可以供用户查看；
- [`Bucket`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/bucket.go#L36-L50) 类似于`表`的一个概念，在boltdb相关数据必须存在一个`Bucket`下，不同`Bucket`下的数据相互隔离，每个`Bucket`下 有一个单调递增的`Sequence`，类似于自增主键；
- [`BucketStats`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/bucket.go#L730-L751) `Bucket`的一些统计信息；
- [`Tx`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/tx.go#L24-L41) `boltdb`的事务数据结构，在`boltdb`中，全部的对数据的操作必须发生在一个事务之内，`boltdb`的并发读取都在此实现；
- [`Cursor`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/cursor.go#L18-L21) 是内部B-TREE的一个迭代器，用于遍历数据库，提供`First`/`Last`/`Seek`等操作；

还有一些内部数据结构，帮助实现一些内部逻辑:

- [`node`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/node.go#L11-L21) 用来存储每个`Bucket`中的一部分Key-Value，每个`Bucket`中的`node`组织成一个B-TREE；
- [`inode`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/node.go#L597-L602) 记录Key-Value对的数据结构，每个`node`内包含一个`inode`数组，`inode`是K-V在内存中缓存的记录，该记录落到磁盘上 时，记录为`leafPageElement`；
- [`leafPageElement`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/page.go#L110-L115) 磁盘上记录具体Key-Value的索引；
- [`page`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/page.go#L30-L36) 用户记录持久化文件中每个区域的重要信息，同时`page`分为很多种类，不同`page`存储不同的数据信息；

### 3.2.2 mmap

在介绍数据结构之前，先来介绍一下什么是Mmap：

> **mmap是一种内存映射文件的方法，即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用read,write等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间**

boltdb就是使用Mmap将数据库文件的内容映射到了内存中的一块连续区域（mmap buffer），每次在内存中对mmap buffer修改以后再同步就可以写入文件之中。

### 3.2.3 图解

![内存bucket数据结构](https://ryanyux.github.io/post/go-blotdb-source/bucket.jpg)page id: 该Bucket：所对应的page的id，之前说过，默认的page大小都是4k，所以我们只需要一个page id就可以计算出这个page在文件中的偏移位置（offset）。 

Page: 一块inline的page，我对这个inline的意思的理解是说，如果一个Bucket比较小，它不需要一块独立的文件上的4k大小的区域，那就直接作为上层bucket的一个value。上文中说过，对于bucket来说，它能看得见的只有key和value，只是如果某一个key-value代表的是一个子bucket，那么它只需要将这个value解析为bucket的数据结构即可，再去读这个子bucket的page id，就可以知道子bucket的所在位置。所以如果这个子bucket如果使用的是一个inline类型的page，那么这个子bucket的page就在其父bucket的value之中。**在实际使用中，所有叶节点的bucket都是inline的** RootNode：代表的是这个bucket的数据，它内部保存着一系列的inode，从图中可以看见这些inode的内部保存着key和value，还有其所在的page的id。所以一个inode就代表了一条数据。 

buckets：这个bucket的子bucket

==流程：当打开一个bucket后，会先找到bucket对应的page，然后读取其中的内容的时候，并赋值给inode中，这样就在内存中生成了一个对应的结构。当需要在这个bucket中再打开一个bucket的时候，就会读inode中的数据，然后解析成bucket的数据结构并生成对象，然后将这个对象放在父bucket的buckets切片中。==

**也可以知道node当中的数据是page中的子集**



# 4. boltdb 持久化

`boltdb`采用一个单独的文件作为持久化存储。其将不同的文件部分划分为不同的page，不同的page 存储不同类型的数据(meta、key、value)等。当经过反复的增删改查后，文件中可能出现没有数据的 部分。此时`boltdb`并不打算搬移数据、截断文件来将多余的空间返还给操作系统。而是将这些部分， 加入内部的`FreeList`来维护，当有新的数据写入时，复用这些空间。因此从用户视角来看，`boltdb` 的持久化文件只会增大，而不会因为数据的删除而减少。

`boltdb`在写入数据时，为了提高效率，采用`unsafe`的方法直接获得相关`struct`的系统内存layout， 而没有使用一些序列化工具。因此持久化文件里记录的数据格式是操作系统相关的。因此不能将一个大端 系统中的持久化文件复制到一个小端系统中使用。

由于`boltdb`采用B-TREE索引，会带来随机写入，所以在有写入瓶颈的场景中，只能通过使用SSD来提高。

当用户写入一个key-value对时，`boltdb`根据key写入相关的B-TREE索引指定的文件偏移量中去。

其零拷贝的特性主要体现在它将持久化文件以只读模式通过`mmap`映射到内存空间中，然后通过索引 找到内存中key对应的value所指向的空间，然后将这段内存返回给用户。因此在database存储数据比较多 时，可能会显示程序的内存使用量很高。`boltdb`在做文件`mmap`映射时的策略比较简单，即将文件整体映 射到内存空间中，完全利用操作系统的虚拟内存空间与物理内存动态映射、换出的机制，而没有去做动态的 映射变更，在地址空间比较小的32bit系统中可能会带来一些限制。

用只读默认来映射也保护了持久化文件，防止用户程序出现内存越界，将持久化文件污染。

这也带来一个问题，`boltdb`返回的内存切片是一段只读的内存，用户不能对其进行修改。而且该内存的生命周期 只存在于相关事务持续周期内。

在前面简介部分已经描述了一部分持久化相关的[内容](https://lrita.github.io/2017/05/21/boltdb-overview-0/#%E6%8C%81%E4%B9%85%E5%8C%96)

`boltdb`采用单个文件来将数据存储在磁盘上，该文件的前4个`page`是固定的:

1. 第1个page为meta
2. 第2个page为meta
3. 第3个page是freelist，存储了一个int数组，
4. 第4个page是leaf page

## 4.1 page

`page`是`boltdb`持久化时，与磁盘相关的数据结构。page的大小采用操作系统内存页的大小，即`getpagesize`系统调用 的返回值，通常是4k大小。

每个page开始的几个字节存储的是[`page`](https://github.com/boltdb/bolt/blob/e9cf4fae01b5a8ff89d0ec6b32f0d9c9f79aefdd/page.go#L30-L36) 的raw data:

```go
type page struct {
    id       pgid         // page的序号
    flags    uint16       // page的具体数据类型，有branchPageFlag/leafPageFlag/metaPageFlag/freelistPageFlag几种
    count    uint16       // 记录具体数据类型中的计数。不同类型不同含义。当page是freelistPageFlag类型时，存储的是freelist中pgid数组中元素的个数；
                          // 当page时其他类型时，存储的是inode的个数
    overflow uint32       // 当写操作数据量大于1个page大小时，该字段记录超出的page数，例如：写入12k的数据，
                          // page大小为4k，则page.overflow=2，标明page header后的2个page大小区域也是该page的
                          // 区域。
    ptr      uintptr      //具体的数据类型，类比c
}
=========================================================================
//页的数据类型
const (
	branchPageFlag   = 0x01  //分支节点，只包含k数据
	leafPageFlag     = 0x02	 //叶子节点，包含kv数据
    //以上用书结构管理页的关系
	metaPageFlag     = 0x04  
	freelistPageFlag = 0x10
)
// typ returns a human readable page type string used for debugging.
func (p *page) typ() string {
	if (p.flags & branchPageFlag) != 0 {
		return "branch"
	} else if (p.flags & leafPageFlag) != 0 {
		return "leaf"
	} else if (p.flags & metaPageFlag) != 0 {
		return "meta"
	} else if (p.flags & freelistPageFlag) != 0 {
		return "freelist"
	}
	return fmt.Sprintf("unknown<%02x>", p.flags)
}
```

每个`page`对应对应一个磁盘上的数据块。这个数据块的layout为:

```
| page struct data | page element items | k-v pairs |
```

其分为3个部分：

- 第一部分`page struct data`是该`page`的header，存储的就是`page`struct的数据。
- 第二部分`page element items`其实就是`node`(下面会讲)的里`inode`的持久化部分数据。
- 第三部分`k-v pairs`存储的是`inode`里具体的key-value数据。

```go
type meta struct {
    magic    uint32  // 存储魔数0xED0CDAED
    version  uint32  // 标明存储格式的版本，现在是2
    pageSize uint32  // 标明每个page的大小
    flags    uint32  // 当前已无用
    root     bucket  // 根Bucket
    freelist pgid    // 标明当前freelist数据存在哪个page中
    pgid     pgid    //
    txid     txid    //
    checksum uint64  // 以上数据的校验和，校验数据是否损坏
}
```

## 4.2 meta

```go
type meta struct {
	magic    uint32
	version  uint32
	pageSize uint32
	flags    uint32
	root     bucket
	freelist pgid
	pgid     pgid
	txid     txid
	checksum uint64
}
```

## 4.3 freelist

根据前面[简介](https://lrita.github.io/2017/05/21/boltdb-overview-0/#%E6%8C%81%E4%B9%85%E5%8C%96)中的描述，`boltdb` 是不会释放空间的磁盘空间的，因此需要一个机制来实现磁盘空间的重复利用，`freelist`就是实现该机制的 文件page缓存。其数据结构为：

```go
type freelist struct {
    ids     []pgid          // all free and available free page ids.
    pending map[txid][]pgid // mapping of soon-to-be free page ids by tx.
    cache   map[pgid]bool   // fast lookup of all free and pending page ids.
}
```

其有三部分组成，`ids`记录了当前缓存着的空闲`page`的pgid，`cache`中记录的也是这些pgid，采用map记录 方便快速查找。

当用户需要`page`时，调用`freelist.allocate(n int) pgid`，其中n为需要的`page`数量，其会遍历`ids`，从中 挑选出连续n个空闲的`page`，然后将其从缓存中剔除，然后将其实的page-id返回给调用者。当不存在满足需求的 `page`时，返回0，因为文件的起始2个`page`固定为meta page，因此有效的page-id不可能为0。

当某个写事务产生无用`page`时，将调用`freelist.free(txid txid, p *page)`将指定`page` p放入`pending`池和 `cache`中。当下一个写事务开启时，会将没有`Tx`引用的`pending`中的`page`搬移到`ids`缓存中。之所以这样做， 是为了支持事务的回滚和并发读事务，从而实现MVCC。

当发起一个读事务时，`Tx`单独复制一份`meta`信息，从这份独有的`meta`作为入口，可以读出该`meta`指向的数据， 此时即使有一个写事务修改了相关key的数据，新修改的数据只会被写入新的`page`，读事务持有的`page`会进入`pending` 池，因此该读事务相关的数据并不会被修改。只有该`page`相关的读事务都结束时，才会从`pending`池进入到`cache`池 中，从而被复用修改。

当写事务更新数据时，并不直接覆盖老数据，而且分配一个新的`page`将更新后的数据写入，然后将老数据占用的`page` 放入`pending`池，建立新的索引。当事务需要回滚时，只需要将`pending`池中的`page`释放，将索引回滚即完成数据的 回滚。这样加速了事务的回滚。减少了事务缓存的内存使用，同时避免了对正在读的事务的干扰。



# 5. 内存结构

## 5.1 node

node为内存中数据的存储模式，page则是磁盘存储格式

```go
// node represents an in-memory, deserialized page.
type node struct {
	bucket     *Bucket  //类似表
	isLeaf     bool  //page的两种数据结构分支和叶子
	unbalanced bool
	spilled    bool
	key        []byte
	pgid       pgid
	parent     *node
	children   nodes
	inodes     inodes  //存储全部kv的结构
}

// inode represents an internal node inside of a node.
// It can be used to point to elements in a page or point
// to an element which hasn't been added to a page yet.
type inode struct {
	flags uint32
	pgid  pgid
	key   []byte
	value []byte
}
type inodes []inode
```

包含具体数据操作的行为，增删改查等。

重要的几个操作：

- `func (n *node) read(p *page)`: 读取磁盘page，进行初始化，加载到内存node
- `(n *node) write(p *page)`: 写入到一个page或多个
- 其他：1）子，兄弟，根节点相关基础操作 ；2）split操作； 3）`put`插入更新操作

## 5.2 bucket

```go
// Bucket represents a collection of key/value pairs inside the database.
type Bucket struct {
	*bucket
	tx       *Tx                // the associated transaction
	buckets  map[string]*Bucket // subbucket cache
	page     *page              // inline page reference
	rootNode *node              // materialized node for the root page.
	nodes    map[pgid]*node     // node cache

	// Sets the threshold for filling nodes when they split. By default,
	// the bucket will fill to 50% but it can be useful to increase this
	// amount if you know that your write workloads are mostly append-only.
	//
	// This is non-persisted across transactions so it must be set in every Tx.
	FillPercent float64
}
```

对node进行初始化

# 6. B-Tree 索引

## 6.1 Cursor

`Cursor`是遍历`Bucket`的迭代器，其声明是:

```go
type elemRef struct {
    page  *page
    node  *node
    index int
}

type Cursor struct {
	bucket *Bucket     // parent Bucket
	stack  []elemRef   // 遍历过程中记录走过的page-id或者node，elemRef中的page、node同时只能有一个存在
}

type Bucket struct {
    *bucket
    tx       *Tx                // the associated transaction
    buckets  map[string]*Bucket // subbucket cache
    page     *page              // inline page reference
    rootNode *node              // materialized node for the root page.
    nodes    map[pgid]*node     // node cache
    FillPercent float64
}

type node struct {
    bucket     *Bucket
    isLeaf     bool	// 标记该节点是否为叶子节点，决定inode中记录的是什么
    unbalanced bool	// 当该node上有删除操作时，标记为true，当Tx执行Commit时，会执行rebalance，将inode重新排列
    spilled    bool
    key        []byte	// 当加载page变成node缓存时，将该node下边界inode[0]的key缓存在node上，用于在parent node
                        // 查找本身时使用
    pgid       pgid
    parent     *node
    children   nodes
    inodes     inodes
}

type inode struct {
    flags uint32   // 当所在node为叶子节点时，记录key的flag
    pgid  pgid     // 当所在node为叶子节点时，不使用，当所在node为分支节点时，记录所指向的page-id
    key   []byte   // 当所在node为叶子节点时，记录的为拥有的key；当所在node为分支节点时，记录的是子
                   // 节点的上key的下边界。例如，当当前node为分支节点，拥有3个分支，[1,2,3][5,6,7][10,11,12]
                   // 这该node上可能有3个inode，记录的key为[1,5,10]。当进行查找时2时，2<5,则去第0个子分支上继
                   // 续搜索，当进行查找4时，1<4<5,则去第1个分支上去继续查找。
    value []byte   // 当所在node为叶子节点时，记录的为拥有的value
}

type bucket struct {
    root     pgid   // page id of the bucket's root-level page
    sequence uint64 // monotonically incrementing, used by NextSequence()
}
```

`boltdb`通过B-Tree来构建数据的索引，其B-Tree的根为`Bucket`，其数上的每个节点为`node`和`inode`或 

`branchPageElements`和`leafPageElement`；B-Tree的上全部的Key-Value数据都记录在叶子节点上。

当`Bucket`的数据还没有commit写入磁盘时，在内存中以`node`和`inode`来记录，当数据被写入磁盘时，以 `branchPageElements`和`leafPageElement`来记录。

正式这样的混合组织方式，因此在搜索这个B-Tree的时候，遇见的数据可以是磁盘上的数据也可能是内存中的数据， 因此采用`Cursor`这样的迭代器。`Cursor`的`stack []elemRef`中几率的每次迭代操作是走过的路径。其路径可能是 磁盘中的某个`page`也可能是还未刷入磁盘的`node`。`elemRef`中的`index`用来记录search时命中的index。

这棵大B-Tree上总共有2中节点，一种是`Bucket`，一种是`node`，这两种不同是节点都存储在B-Tree的K-V对上，只是 flag不同。`Bucket`被当做树或者子树的根节点，`node`是B-Tree上的普通节点，依负在某一个`Bucket`上。`Bucket` 当做一个子树看待，所以不会跟同级的`node`一同`rebalance`。`Bucket`在树上记录的value为`bucket`，即根节点的 page-id和sequence。

## 6.2 Bucket

因此，很好理解`Bucket`的嵌套关系。子`Bucket`就是在父`Bucket`上创建一个`Bucket`节点。 关于`Bucket`的描述可以参考[BoltDB之Bucket(一)](http://www.d-kai.me/boltdb%E4%B9%8Bbucket%E4%B8%80/)/[BoltDB之Bucket(二)](http://www.d-kai.me/boltdb%E4%B9%8Bbucket%E4%BA%8C/) 两篇文章的描述。看这2篇文章时，先忽略掉`inline bucket`部分的内容，不然不容易理解。`inline bucket`只不过是 一个对于磁盘空间的优化，通常`Bucket`的信息在磁盘上的记录很小，如果直接占用一个`page`有些浪费，则将`Bucket` 对应`page`的剩余部分当做`Bucket`可以使用的一个`page`，则当数据量较小时，可以节省一个`page`。

`node`并不是B-Tree上的一个节点，并不是最总存储数据的K-V对，在`node`上的`inode`才是最终存储数据的K-V对。 每个`node`对应唯一一个`page`，是`page`在内存中的缓存对象。在`Bucket`下会有`node`的缓存集合。当需要访问 某个`page`时，会先去缓存中查找其`node`，只有`node`不存在时，才去加载`page`。

## 6.3 MVCC

`bolt`的MVCC实现主要依赖COW和`meta`副本来实现。

每当一个`Tx`被创建时，就复制一份当前最新的`meta`。在`Tx`中的每个操作都会将变化后的B-Tree上的`node`缓存 `Tx`的`Bucket`副本中，这个变化只对该`Tx`可见。当有删除操作时，`Tx`会将要释放的page-id暂存在`freelist`的 `pending`池中，当`Tx`调用Commit时，`freelist`会将`pending`的page-id真正标记为free状态，如果`Tx`调用`Rollback` 则会将`pending`池中的page-id移除，则使`Tx`的删除操作回滚。

当`Tx`调用`Commit`时，

会触发`Bucket`的`rebalance`，`rebalance`会根据阈值条件，尽量提高每个修改过的`node`的 使用率(即`Bucket`缓存的`node`，只有`put`/`del`过的`page`才会加载成`node`缓存在`Bucket`下)，经过剪去空数据分 枝、合并相邻且填充率较低的分支，最后通过数据的搬移，释放更多的`page`。

然后再触发`Bucket`的`spill`，`spill`会再将`rebalance`聚拢后的`node`根据`Bucket.FillPercent`将每个`node`所持 有的数据将到`pagesize*Bucket.FillPercent`之下。然后获取新的`page`(获取新的`page`时，会先去freelist查找能复用 page-id，如果找不到就新分配一个，当新分配的`page`使文件大小超过只读mmap的大小时，会重新mmap一次)，然后将`node` 写入`page`。

然后更新`freelist`的持久化记录。然后将上面操作的`page`刷新到磁盘上。最后更新`meta`的在磁盘上的记录、释放`Tx` 的资源。

因此在没有更新`meta`之前，写入的数据对于读事务都是不可见的。

### 文件映射增长策略

当`boltdb`文件小于1GB时，每次重新映射时，映射大小翻倍，当文件大于1GB时，每次增长1GB，且与pagesize对齐。

# 7. 事务

```go
// Tx represents a read-only or read/write transaction on the database.
// Read-only transactions can be used for retrieving values for keys and creating cursors.
// Read/write transactions can create and remove buckets and create and remove keys.
//
// IMPORTANT: You must commit or rollback transactions when you are done with
// them. Pages can not be reclaimed by the writer until no more transactions
// are using them. A long running read transaction can cause the database to
// quickly grow.
type Tx struct {
	writable       bool
	managed        bool
	db             *DB
	meta           *meta
	root           Bucket
	pages          map[pgid]*page
	stats          TxStats
	commitHandlers []func()

	// WriteFlag specifies the flag for write-related methods like WriteTo().
	// Tx opens the database file with the specified flag to copy the data.
	//
	// By default, the flag is unset, which works well for mostly in-memory
	// workloads. For databases that are much larger than available RAM,
	// set the flag to syscall.O_DIRECT to avoid trashing the page cache.
	WriteFlag int
}
```



# 参考

- [持久化文件增长和复用](https://github.com/boltdb/bolt/issues/308#issuecomment-74811638)