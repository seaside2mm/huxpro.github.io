# OLTP

PARALLEL VS. DISTRIBUTED
Parallel DBMSs:
→ Nodes are physically close to each other.
→ Nodes connected with high-speed LAN.
→ Communication cost is assumed to be small.
Distributed DBMSs:
→ Nodes can be far from each other.
→ Nodes connected using public network.
→ Communication cost and problems cannot be ignored.





DISTRIBUTED DBMSs
Use the building blocks that we covered in singlenode DBMSs to now support transaction
processing and query execution in distributed
environments.
→ Optimization & Planning
→ Concurrency Control
→ Logging & Recovery



OLTP VS. OL AP
On-line Transaction Processing (OLTP):
→ Short-lived read/write txns.
→ Small footprint.
→ Repetitive operations.
On-line Analytical Processing (OLAP):
→ Long-running, read-only queries.
→ Complex joins.
→ Exploratory queries.





## System Architectures

A DBMS's system architecture specifies what shared resources are directly accessible to CPUs. This affects how CPUs coordinate with each other and where they retrieve/store objects in the database.

### SHARED MEMORY

CPUs have access to common  memory address space via a fast interconnect.

- Each processor has a global view of all the in-memory data structures.
- Each DBMS instance on a processor has to "know" about the other instances 

### SHARED DISK
All CPUs can access a single logical disk directly via an interconnect but each have their own private memories.

- Can scale execution layer independently from the storage layer.
-  Have to send messages between CPUs to learn about their current state 

### SHARED NOTHING
Each DBMS instance has its own CPU, memory, and disk.
Nodes only communicate with each other via network.

- Easy to increase capacity.
-  Hard to ensure consistency 

## Design Issues

How does the application find data?
How to execute queries on distributed data?
==→ Push query to data.==
==→ Pull data to query.==
How does the DBMS ensure correctness?

### HOMOGENOUS VS. HETEROGENOUS

Approach #1: Homogenous Nodes
→ Every node in the cluster can perform the same set of tasks (albeit on potentially different partitions of data).
→ Makes provisioning and failover "easier".
Approach #2: Heterogenous Nodes
→ Nodes are assigned specific tasks.
→ Can allow a single physical node to host multiple “virtual" node types for dedicated tasks



DATA TRANSPARENCY
Users should not be required to know where data is physically located, how tables are partitioned or replicated.
A SQL query that works on a single-node DBMS should work the same on a distributed DBMS.

### DATABASE PARTITIONING
Split database across multiple resources:
→ Disks, nodes, processors.
→ Sometimes called "sharding"
The DBMS executes query fragments on each partition and then combines the results to produce a single answer.

#### NAÏVE TABLE PARTITIONING
Each node stores one and only table.
Assumes that each node has enough storage space for a table。

#### HORIZONTAL PARTITIONING
Split a table's tuples into disjoint subsets.
→ Choose column(s) that divides the database equally in terms of size, load, or usage.
→ Each tuple contains all of its columns.
→ Hash Partitioning, Range Partitioning
The DBMS can partition a database physical (shared nothing) or logically (shared disk).

- LOGICAL PARTITIONING

- PHYSICAL PARTITIONING

SINGLE-NODE VS. DISTRIBUTED
A single-node txn only accesses data that is contained on one partition.
→ The DBMS does not need coordinate the behavior concurrent txns running on other nodes.
A distributed txn accesses data at one or more partitions.
→ Requires expensive coordination.

TRANSACTION COORDINATION
If our DBMS supports multi-operation and distributed txns, we need a way to coordinate their execution in the system.
Two different approaches:
→ Centralized: Global "traffic cop".
→ Decentralized: Nodes organize themselves.

## Partitioning Schemes

## Distributed Concurrency Control

Need to allow multiple txns to execute simultaneously across multiple nodes.
→ Many of the same protocols from single-node DBMSs can be adapted.
This is harder because of:
→ Replication.
→ Network Communication Overhead.
→ Node Failures.
→ Clock Skew.



We have not discussed how to ensure that all nodes agree to commit a txn and then to make sure it does commit if we decide that it should.
→ What happens if a node fails?
→ What happens if our messages show up late?

### ATOMIC COMMIT PROTOCOL

When a multi-node txn finishes, the DBMS needs to ask all of the nodes involved whether it is safe to commit.
→ All nodes must agree on the outcome
Examples:
→ Two-Phase Commit
→ Three-Phase Commit (not used)
→ Paxos
→ Raft
→ ZAB (Apache Zookeeper)



#### 2PC OPTIMIZATIONS
**Early Prepare Voting**
→ If you send a query to a remote node that you know will be the last one you execute there, then that node will also return their vote for the prepare phase with the query result.
**Early Acknowledgement After Prepare**
→ If all nodes vote to commit a txn, the coordinator can send the client an acknowledgement that their txn was successful before the commit phase finishes

#### TWO-PHASE COMMIT
Each node has to record the outcome of each phase in a stable storage log.
What happens if coordinator crashes?
→ Participants have to decide what to do.
What happens if participant crashes?
→ Coordinator assumes that it responded with an abort if it hasn't sent an acknowledgement yet. The nodes have to block until they can figure out the correct action to take.

#### PAXOS
Consensus protocol where a coordinator proposes an outcome (e.g., commit or abort) and then the participants vote on whether that outcome should
succeed.
Does not block if a majority of participants are available and has provably minimal message delays in the best case.
→ First correct protocol that was provably resilient in the face asynchronous networks

#### 2PC VS. PAXOS
Two-Phase Commit
→ Blocks if coordinator fails after the prepare message is sent, until coordinator recovers.
Paxos
→ Non-blocking as long as a majority participants are alive, provided there is a sufficiently long period without further failures.



# OLAP

**DECISION SUPPORT SYSTEMS**
Applications that serve the management, operations, and planning levels of an organization to help people make decisions about future issues and problems by analyzing historical data.



## Star Schema vs. Snowflake Schema

Issue #1: Normalization
→ Snowflake schemas take up less storage space.
→ Denormalized data models may incur integrity and
consistency violations.
Issue #2: Query Complexity
→ Snowflake schemas require more joins to get the data
needed for a query.
→ Queries on star schemas will (usually) be faster.



## Execution Models

### PUSH VS. PULL
Approach #1: Push Query to Data
→ Send the query (or a portion of it) to the node that
contains the data.
→ Perform as much filtering and processing as possible
where data resides before transmitting over network.
Approach #2: Pull Data to Query
→ Bring the data to the node that is executing a query that
needs it for processing



### FAULT TOLERANCE
Traditional distributed OLAP DBMSs were designed to assume that nodes will not fail during
query execution.
→ If the DBMS fails during query execution, then the whole query fails.
The DBMS could take a snapshot of the intermediate results for a query during execution to allow it to recover after a crash

## Query Planning

All the optimizations that we talked about before are still applicable in a distributed environment.
→ Predicate Pushdown
→ Early Projections
→ Optimal Join Orderings
But now the DBMS must also consider the location of data at each partition when optimizing

### QUERY PLAN FRAGMENTS
Approach #1: **Physical Operators**
→ Generate a single query plan and then break it up into partition-specific fragments.
→ Most systems implement this approach.
Approach #2: **SQL**
→ Rewrite original query into partition-specific queries.
→ Allows for local optimization at each node.
→ MemSQL is the only system that I know that does this



The efficiency of a distributed join depends on the target tables' partitioning schemes.
One approach is to put entire tables on a single node and then perform the join.
→ You lose the parallelism of a distributed DBMS.
→ Costly data transfer over the network.





## Distributed Join Algorithms

To join tables R and S, the DBMS needs to get the proper tuples on the same node. Once there, it then executes the same join algorithms that we discussed earlier in the semester

**SCENARIO #1**
One table is replicated at every node. Each node joins its local data and then sends their results to a coordinating node.

**SCENARIO #2**
Tables are partitioned on the join attribute. Each node performs the join on local data and then sends to a node for coalescing

**SCENARIO #3**
Both tables are partitioned on different keys. If one of the tables is small, then the DBMS **broadcasts** that table to all nodes.

**SCENARIO #4**
Both tables are not partitioned on the join key. The DBMS copies the tables by **reshuffling** them across nodes.



**REL ATIONAL ALGEBRA: SEMI-JOIN**
Like a natural join except that the attributes that are not used to compute the join are restricted.

Distributed DBMSs use semi-join to minimize a3 103 the amount of data sent during joins.
This is the same as a projection pushdown.

## Cloud Systems

Vendors provide database-as-a-service (DBaaS)
offerings that are managed DBMS environments.
Newer systems are starting to blur the lines
between shared-nothing and shared-disk

Approach #1: Managed DBMSs
→ No significant modification to the DBMS to be "aware"
that it is running in a cloud environment.
→ Examples: Most vendors
Approach #2: Cloud-Native DBMS
→ The system is designed explicitly to run in a cloud
environment.
→ Usually based on a shared-disk architecture.
→ Examples: Snowflake, Google BigQuery, Amazon
Redshift, Microsoft SQL Azure

UNIVERSAL FORMATS
Traditional DBMSs store data in proprietary
binary file formats that are incompatible.
One can use text formats (XML/JSON/CSV) to
share data across different systems.
There are now standardized file formats.

Apache Parquet
→ Compressed columnar storage from Cloudera/Twitter
Apache ORC
→ Compressed columnar storage from Apache Hive.
HDF5
→ Multi-dimensional arrays for scientific workloads.
Apache Arrow
→ In-memory compressed columnar storage from Pandas/Dremio



# Distributed Query Processing



# Distributed Directory Systems
## **LDAP: Lightweight Directory Access Protocol**